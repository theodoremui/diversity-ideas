{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Columbia with Spider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from pprint import pprint\n",
    "from spider import Spider\n",
    "\n",
    "import asyncio\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Spider()\n",
    "\n",
    "urls = [\n",
    "    \"https://www.columbiaspectator.com/opinion/op-eds/1/\",\n",
    "    \"https://www.columbiaspectator.com/opinion/2024/12/11/at-columbia-we-dont-strike-our-ideological-opponents/\",\n",
    "    \"https://www.columbiaspectator.com/opinion/2024/12/08/columbias-complicity-in-cop29-the-greenwashing-of-human-rights-abuses/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_params = {\n",
    "    \"limit\": 1,\n",
    "    \"metadata\": True,\n",
    "    \"request\": \"smart\",\n",
    "    # \"smart_mode\": True,\n",
    "    \"respect_robots\": False,\n",
    "    \"return_format\": \"markdown\",\n",
    "    \"anti_bot\": True,\n",
    "    \"stealth\": True,\n",
    "    \"fingerprint\": True,\n",
    "    \"readability\": True,\n",
    "    \"scroll\": 500,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_url(url: str) -> str:\n",
    "    return app.scrape_url(url, params=spider_params)\n",
    "\n",
    "\n",
    "async def scrape_urls(urls: List[str]) -> List[str]:\n",
    "    return await asyncio.gather(*[scrape_url(url) for url in urls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = app.scrape_url(urls[0], params=spider_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(md), md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider Web Reader with LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.readers.web.spider_web.base import (\n",
    "    SpiderWebReader,\n",
    ")\n",
    "SPIDER_API_KEY = os.getenv(\"SPIDER_API_KEY\")\n",
    "spider_reader = SpiderWebReader(\n",
    "    api_key=SPIDER_API_KEY,  # Get one at https://spider.cloud\n",
    "    mode=\"crawl\",\n",
    "    params=spider_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.6 ms, sys: 4.29 ms, total: 13.9 ms\n",
      "Wall time: 32.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "documents = spider_reader.load_data(url=urls[0])\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding': None,\n",
      " 'end_char_idx': None,\n",
      " 'excluded_embed_metadata_keys': [],\n",
      " 'excluded_llm_metadata_keys': [],\n",
      " 'id_': '32a91511-3e1f-40f1-82dc-55603a00a4db',\n",
      " 'metadata': {'description': 'Why are we at Columbia, and what is the purpose '\n",
      "                             'of higher education? Many of us arrived at '\n",
      "                             'Columbia fueled by curiosity and a yearning for '\n",
      "                             'knowledge. However, in recent months, it has '\n",
      "                             'become clear that not all members of our '\n",
      "                             'community share a vision of open dialogue and '\n",
      "                             'mutual learning.',\n",
      "              'domain': 'www.columbiaspectator.com',\n",
      "              'extracted_data': None,\n",
      "              'file_size': 21517,\n",
      "              'keywords': None,\n",
      "              'pathname': '/opinion/2024/12/11/at-columbia-we-dont-strike-our-ideological-opponents/',\n",
      "              'resource_type': '.md',\n",
      "              'title': 'At Columbia, we don’t ‘strike’ our ideological '\n",
      "                       'opponents',\n",
      "              'url': None,\n",
      "              'user_id': 'e305c83a-df60-4de2-aaf9-721cd50c71da'},\n",
      " 'metadata_seperator': '\\n',\n",
      " 'metadata_template': '{key}: {value}',\n",
      " 'mimetype': 'text/plain',\n",
      " 'relationships': {},\n",
      " 'start_char_idx': None,\n",
      " 'text': '',\n",
      " 'text_template': '{metadata_str}\\n\\n{content}'}\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_crawler = SpiderWebReader(\n",
    "    api_key=SPIDER_API_KEY,  \n",
    "    mode=\"crawl\",\n",
    "    params=spider_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.49 ms, sys: 4.32 ms, total: 12.8 ms\n",
      "Wall time: 37.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "documents = spider_crawler.load_data(url=urls[0])\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(vars(documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPressScraper:\n",
    "    def __init__(self, url, scroll_pause_time=2):\n",
    "        self.url = url\n",
    "        self.scroll_pause_time = scroll_pause_time\n",
    "        \n",
    "        # Add Chrome options\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')  # Run in headless mode (optional)\n",
    "        \n",
    "        # Create service object with specific driver path if needed\n",
    "        # Specify the path to your ChromeDriver\n",
    "        chromedriver_path = \"/Users/pmui/.local/chromedriver-mac-arm64/chromedriver\"  # Replace with your actual path\n",
    "        service = webdriver.ChromeService(executable_path=chromedriver_path)\n",
    "        # service = webdriver.ChromeService()\n",
    "        \n",
    "        self.driver = webdriver.Chrome(options=options, service=service)\n",
    "        \n",
    "    def scroll_to_bottom(self, max_scrolls=None):\n",
    "        \"\"\"Scroll to the bottom of the page to trigger content loading\"\"\"\n",
    "        scrolls = 0\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            # Scroll down\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            \n",
    "            # Wait for new content to load\n",
    "            time.sleep(self.scroll_pause_time)\n",
    "            \n",
    "            # Calculate new scroll height\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            # Break if reached bottom or max scrolls\n",
    "            if new_height == last_height or (max_scrolls and scrolls >= max_scrolls):\n",
    "                break\n",
    "                \n",
    "            last_height = new_height\n",
    "            scrolls += 1\n",
    "            \n",
    "        return scrolls\n",
    "    \n",
    "    def extract_main_content(self):\n",
    "        \"\"\"Extract main content from the page\"\"\"\n",
    "        # Common WordPress article content selectors\n",
    "        content_selectors = [\n",
    "            'article',\n",
    "            '.post-content',\n",
    "            '.entry-content',\n",
    "            '.post',\n",
    "            'main',\n",
    "            '#main-content'\n",
    "        ]\n",
    "        \n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        articles = []\n",
    "        \n",
    "        # Try different selectors\n",
    "        for selector in content_selectors:\n",
    "            content_elements = soup.select(selector)\n",
    "            if content_elements:\n",
    "                for element in content_elements:\n",
    "                    # Extract text and clean it\n",
    "                    text = element.get_text(strip=True, separator=' ')\n",
    "                    if text:  # Only add non-empty content\n",
    "                        article = {\n",
    "                            'text': text,\n",
    "                            'url': element.find('a', href=True)['href'] if element.find('a', href=True) else None,\n",
    "                            'title': element.find('h1', class_='entry-title').text if element.find('h1', class_='entry-title') else None\n",
    "                        }\n",
    "                        articles.append(article)\n",
    "                break  # Stop if we found content with current selector\n",
    "                \n",
    "        return articles\n",
    "    \n",
    "    def scrape(self, max_scrolls=None):\n",
    "        \"\"\"Main scraping method\"\"\"\n",
    "        try:\n",
    "            # Load the page\n",
    "            self.driver.get(self.url)\n",
    "            \n",
    "            # Wait for any of these elements to be present\n",
    "            selectors = [\n",
    "                (By.TAG_NAME, \"article\"),\n",
    "                (By.CLASS_NAME, \"post-content\"),\n",
    "                (By.CLASS_NAME, \"entry-content\"),\n",
    "                (By.TAG_NAME, \"main\"),\n",
    "                (By.ID, \"main-content\")\n",
    "            ]\n",
    "            \n",
    "            # Try each selector with a longer timeout\n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    WebDriverWait(self.driver, 20).until(\n",
    "                        EC.presence_of_element_located(selector)\n",
    "                    )\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Add a small delay to ensure content is loaded\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Scroll to load all content\n",
    "            total_scrolls = self.scroll_to_bottom(max_scrolls)\n",
    "            \n",
    "            # Extract content\n",
    "            articles = self.extract_main_content()\n",
    "            \n",
    "            return {\n",
    "                'total_scrolls': total_scrolls,\n",
    "                'articles': articles\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'total_scrolls': 0,\n",
    "                'articles': []\n",
    "            }\n",
    "            \n",
    "        finally:\n",
    "            self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.columbiaspectator.com/opinion/op-eds/1/\",\n",
    "    \"https://www.columbiaspectator.com/opinion/2024/12/11/at-columbia-we-dont-strike-our-ideological-opponents/\",\n",
    "    \"https://www.columbiaspectator.com/opinion/2024/12/08/columbias-complicity-in-cop29-the-greenwashing-of-human-rights-abuses/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = WordPressScraper(urls[1])\n",
    "results = scraper.scrape(max_scrolls=1)  \n",
    "\n",
    "# Save results to file\n",
    "with open('scraped_content.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
